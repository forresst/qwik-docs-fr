---
title: Qwik City Parallélisation des requêtes réseau
contributors:
  - mhevery
  - adamdbradley
---

# Parallélisation des requêtes réseau

Dans les docs de [Mise en cache des paires de requêtes et de réponses](../request-response-cache/index.mdx), nous avons expliqué la puissante combinaison du [Cache](https://developer.mozilla.org/en-US/docs/Web/API/Cache) et du [Service Worker](https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorker). Cependant, nous pouvons aller encore plus loin en veillant à ce que les requêtes en double ne soient pas créées pour le même bundle, et nous pouvons empêcher les phénomènes de cascade sur le réseau.

## Éviter les requêtes en double

Par exemple, disons qu'un utilisateur final dispose actuellement d'une connexion 3G lente. Quand il demande la page d'accueil, aussi vite que le permet ce réseau lent, l'appareil charge le HTML et rend le contenu (un domaine où Qwik se distingue vraiment). Sur cette connexion lente, ce serait une honte s'ils devaient aussi charger quelques centaines de kilobytes de plus juste pour [faire fonctionner leur appli et la rendre interactive] (https://www.builder.io/blog/hydration-is-pure-overhead).

Cependant, parce que l'appli a été construite avec Qwik, l'utilisateur final n'a pas besoin de charger l'application complète pour qu'elle devienne interactive. Au lieu de cela, l'utilisateur final a déjà chargé l'appli HTML rendue par le SSR, et toutes les parties interactives, comme un bouton "Ajouter au panier", peuvent être préchargées immédiatement. Notez que nous ne préchargeons que le code de l'écouteur proprement dit, et _pas_ la pile entière des fonctions de rendu de l'arborescence des composants.

Dans cet exemple extrêmement courant dans le monde réel d'un appareil avec une connexion lente, l'appareil commence immédiatement à précharger les interactions possibles qui sont visibles par l'utilisateur final. Toutefois, en raison de la lenteur de la connexion, même si nous avons commencé à effectuer la prélecture dès que possible dans un [thread d'arrière-plan] (../service-worker-prefetching/index.mdx), la requête de prélecture elle-même peut encore être en cours.

À des fins de démonstration, disons que la prélecture pour ce bundle prend deux secondes. Cependant, après une seconde d'affichage de la page, l'utilisateur clique sur le bouton. Dans un framework traditionnel, il y a de fortes chances pour qu'il ne se passe absolument rien ! Si le chargement du framework n'est pas terminé, l'appli n'est pas hydratée, l'appli n'a pas été re-rendue et l'écouteur d'événement n'a pas encore été ajouté au bouton. Malheureusement, l'interaction de l'utilisateur serait tout simplement perdue sur un framework utilisant l'hydratation.

Cependant, avec la prélecture et la mise en cache de Qwik, si l'utilisateur a cliqué sur le bouton, et que nous avons déjà commencé une requête il y a une seconde, et qu'il reste une seconde avant qu'elle ne soit complètement reçue, alors l'utilisateur final n'a qu'une seconde à attendre. N'oubliez pas qu'il dispose d'une connexion 3G lente pour cette démonstration. Heureusement, l'utilisateur a déjà reçu le rendu complet de la page d'accueil, donc il voit déjà une page complète. Ensuite, il ne précharge que les parties de l'appli avec lesquelles il peut interagir, et sa connexion lente n'est dédiée qu'à ce(s) bundle(s). Cela contraste avec la lenteur de leur connexion qui charge toutes les applications, juste pour exécuter l'écouteur unique.

Qwik est capable d'intercepter les requêtes pour des bundles connus, et si un préchargement est déjà en cours, et qu'ensuite un utilisateur demande le même bundle, il s'assurera que la seconde requête est capable de réutiliser la première, qui peut déjà être chargée. Faire tout cela avec le [link](../request-response-cache/index.mdx#known-issues-with-link) montre aussi pourquoi Qwik a préféré ne pas le faire par défaut, mais plutôt utiliser l'[API de mise en cache](../request-response-cache/index.mdx).

## Réduire les effets en cascade du réseau

On parle d'effet en cascade sur le réseau lorsque de nombreuses requêtes arrivent les unes après les autres, comme les marches d'un escalier, plutôt qu'en parallèle. Une cascade de requêtes réseau nuit généralement aux performances car elle augmente le temps nécessaire au chargement de tous les modules, alors que le chargement de chaque module débute en même temps.

Voici un exemple avec trois modules : A, B et C. Le module A importe B, et B importe C. C'est le document HTML qui démarre la cascade en demandant tout d'abord le module A.

```ts
import './b.js';
console.log('Module A');
```

```ts
import './c.js';
console.log('Module B');
```

```ts
console.log('Module C');
```

```html
<script type="module" src="./a.js"></script>
```

Dans cet exemple, lorsque le module A est appelé pour la première fois, le navigateur ne sait pas qu'il doit également demander les modules B et C. Il ne sait même pas qu'il doit commencer à demander le module B avant que le module A ne soit terminé. Il ne sait même pas qu'il doit commencer à demander le module `B`, seulement APRÈS que le module `A` ait fini de se charger. Il s'agit d'un problème courant dans la mesure où le navigateur ne sait pas à l'avance ce qu'il doit commencer à demander, jusqu'à ce que le chargement de chaque module soit terminé.

Cependant, comme notre service worker contient un graphe de modules généré à partir du manifeste, nous connaissons tous les modules qui seront demandés ensuite. Ainsi, lors d'une interaction avec l'utilisateur ou d'une recherche préalable d'un bundle, le navigateur lance la demande de tous les bundles qui seront demandés. Cela nous permet de réduire considérablement le temps nécessaire pour demander tous les bundles.
